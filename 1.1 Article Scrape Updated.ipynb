{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrape\n",
    "\n",
    "Since the original dataset was a bit incomplete, I decided to take a look at the HTML myself to fill in some of the missing pieces, as well as collect some new features as well. As you can imagine, this process took a long time so I added multithreading to make the process a bit faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    \"\"\"Creates folders specified in a given path if they don't already exist.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path containing folders to be created.\n",
    "    \"\"\"\n",
    "    final_path = '.'\n",
    "    for folder in path.split('/'):\n",
    "        if folder not in os.listdir(final_path):\n",
    "            os.mkdir(final_path+'/'+folder)\n",
    "        final_path += '/'+folder\n",
    "\n",
    "def fetch(url,session):\n",
    "    \"\"\"Takes article URL and finds the title, text, references, amount of images and videos, topics, publish date, and data channel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        article url\n",
    "    \n",
    "    session:\n",
    "        Idk the code freaks out if I don't include this I'm still trying to figure out why.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        returns a dictionary with the following keys.\n",
    "        'title'  : article title\n",
    "        'text'   : article text\n",
    "        'refs'   : other articles referenced in the text\n",
    "        'images' : amount of images\n",
    "        'videos' : amount of videos\n",
    "        'topics' : all topic keywords\n",
    "        'date'   : publish date\n",
    "        'time'   : publish time\n",
    "        'channel': labeled data channel\n",
    "    \"\"\"\n",
    "    global data, urls, urls_collected\n",
    "    \n",
    "    save_url = url[:]\n",
    "    urls_collected.append(save_url)\n",
    "\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    article = dict()\n",
    "\n",
    "    try: # At some point mashable changed the style of their article urls, this checks \n",
    "         # whether an error page comes up and if so makes the required replacement.\n",
    "        if str(soup.findAll('h1')[0]) == '<h1>The Bad News</h1>':\n",
    "            url_sub = url[:20]+'article'+url[30:]\n",
    "            html = requests.get(url_sub).text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            url = url_sub\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    # get article title\n",
    "    try:\n",
    "        article['title'] = soup.findAll('title')[0].text\n",
    "    except:\n",
    "        article['title'] = np.nan\n",
    "\n",
    "    # get article text and refrences\n",
    "    try:\n",
    "        section = soup.findAll(\"section\", {\"class\":\"article-content\"})[0]\n",
    "        article['text'] = section.text.replace('\\n','')\n",
    "        article['refs'] = [a['href'] for a in section.findAll(\"a\") if 'href' in str(a)]\n",
    "    except:\n",
    "        article['text'],article['refs'] = [np.nan]*2\n",
    "\n",
    "    # get the amount of images\n",
    "    try:\n",
    "        article['images'] = len(soup.findAll('img')[3:-9])\n",
    "    except:\n",
    "        article['images'] = np.nan\n",
    "    # get the amount of videos\n",
    "    try:\n",
    "        article['videos'] = len(soup.findAll('iframe'))\n",
    "    except:\n",
    "        article['videos'] = np.nan\n",
    "\n",
    "    # get all the labeled topics\n",
    "    try:\n",
    "        article['topics'] = [a.text for a in soup.findAll('footer', {'class':'article-topics'})[0].findAll('a')]\n",
    "    except:\n",
    "        article['topics'] = np.nan\n",
    "\n",
    "    # get the publish date and time\n",
    "    try:\n",
    "        article['date'], article['time'] = soup.findAll('time')[0].text.split(\" \")[:-1]\n",
    "    except:\n",
    "        article['date'], article['time'] = [np.nan]*2\n",
    "\n",
    "    # get the labled data channel\n",
    "    try:\n",
    "        article['channel'] = soup.findAll('article',{'class':'full post story'})[0]['data-channel']\n",
    "    except:\n",
    "        article['channel'] = np.nan\n",
    "\n",
    "    # put everything together\n",
    "    data[save_url] = article\n",
    "    print('\\r%d/%d Articles Collected (%.2f%%)' % (len(data),len(urls),len(data)/len(urls) * 100),end=\"\")\n",
    "\n",
    "async def get_data_asynchronus(urls):\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        with requests.Session() as session:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            tasks = [\n",
    "                loop.run_in_executor(\n",
    "                    executor,\n",
    "                    fetch,\n",
    "                    *(url,session),\n",
    "                )\n",
    "                for url in urls\n",
    "            ]\n",
    "            for response in await asyncio.gather(*tasks):\n",
    "                pass\n",
    "\n",
    "def main(urls):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_data_asynchronus(urls))\n",
    "    loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "39522/39644 Articles Collected (99.69%)Process Complete\n"
     ]
    }
   ],
   "source": [
    "# load config json\n",
    "paths = json.load(open('Scrape Config.json'))\n",
    "urls = pd.read_csv(paths['url_path'])['url']\n",
    "save_path = paths['save_path']\n",
    "\n",
    "# empty dict for article entries\n",
    "data = dict()\n",
    "# empty list for all collected urls\n",
    "urls_collected = []\n",
    "\n",
    "while len(urls_collected) != len(urls):\n",
    "    urls_to_be_collected = urls.drop(urls_collected)\n",
    "    main(urls_to_be_collected)\n",
    "    print('\\r%d/%d Articles Collected (%.2f%%)' % (len(data),len(urls),len(data)/len(urls) * 100),end=\"\")\n",
    "print(\"/nProcess Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Data/Articles'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "data = pd.DataFrame(data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}